{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing, model training and testing, and accuracy metics\n",
    "In this exercise we are going to go through the basics of getting machine learning models running via scikit-learn. First, let's start by recreating the vizualizations of the iris plot that I showed in the lecture earlier.\n",
    "\n",
    "Note: the iris vizualization code contains heavily modified code originally written by Richard Ji (formerly found at this link which now seems to be broken: https://shichaoji.com/2017/02/16/famous-iris-dataset-visualization/)\n",
    "\n",
    "You can do this exercise using the jupyter notebook server at https://embo-popgen.recas.ba.infn.it/user/username/lab, replacing 'username' with your username on the server, or run things locally provided you happen to have all of the necessary libraries installed (not recommended for the exercise as you might end up losing too much time battling dependencies).\n",
    "\n",
    "## 1. Import the iris data and plotting essentials (and of course, numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = sklearn.datasets.load_iris()\n",
    "print(iris.data.shape)\n",
    "print(iris.data)\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot a single feature of the iris dataset.\n",
    "\n",
    "Below we are going to create a simple scatter plot of all 150 examples in the iris dataset, colored by class. We are only plotting sepal length here so to better see our data points we are going to add some Gaussian noise to the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 1))\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(iris.data[:, 0], np.random.normal(0, 1, len(iris.data)), c=iris.target, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look very good, and it seems we would have a tough time separating these classes on the basis of this measurement alone. It might be that some of our features do better than this. Give it a shot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your plotting code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add a second feature to our plot.\n",
    "Well, that's nice, but let's see if we can do better by incporating some more information. What happens if we add sepal width into the mix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's see it in 3D!\n",
    "\n",
    "We are going to have to import another plotting tool for this, called `Axes3D`. Then we can add an additional feature to our plot and see if things look even more separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.clf()\n",
    "\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(iris.data[:, 3], iris.data[:, 2], iris.data[:, 1], c=iris.target, cmap=plt.cm.coolwarm, edgecolors='black')\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[3])\n",
    "ax.set_ylabel(iris.feature_names[2])\n",
    "ax.set_zlabel(iris.feature_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's starting to look like we can really make some headway in separating these classes, and we could probably do even better in 4D. But vizualising stuff in 4D is kind of challenging, for humans anyway, and you are probably getting bored of plotting, so let's do some actual machine learning.\n",
    "\n",
    "## 5. Training a _k_-NN classifier on the Iris dataset: preprocessing\n",
    "Let's train a _k_-neareast neighbor classifier. First, we are going to have to preprocess our data as discussed in the lecture to make sure that the different scales of features don't get us into trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# First, reload our iris data in a manner that is more convenient for training/testing\n",
    "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Let's have a look at our y values (labels)\n",
    "print(y)\n",
    "\n",
    "# Next, we rescale our X data. What does this function do?\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split our data into training and test sets. This is super easy to do using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Here we split into training and test data (ps: what's up with this stratify parameter?)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 60% of the data in our training set and 40% in our test set. There is nothing magical about this split ratio. It is totally arbitrary and for larger data sets a smaller fraction in the test set may be sufficient as long as you can estimate your accuracy metrics with a reasonable degree of certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a _k_-NN classifier on the Iris dataset: training and testing\n",
    "\n",
    "Now that we have our training and test data we are ready to do some real work (after some more imports, of course). We will start by training our classifier and computing a very simple performance metric on our test data: accuracy (number of correctly classified examples / total number of examples classified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# This function takes a single parameter: k. Let's start with one.\n",
    "clf = KNeighborsClassifier(1)\n",
    "\n",
    "# Ready to train? here we go!\n",
    "clf.fit(X_train, y_train)\n",
    "# Well that was easy...\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby!\n",
    "\n",
    "But maybe we could do better (or worse!) with different values of k. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work! Or I'm sorry it turned out that way! You know, whichever..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training a _k_-NN classifier on the Iris dataset: cross-validation\n",
    "The fact is, we really don't have a lot of data here. This means we have very little information to train on, in which case we may not be generalizing as well as we would like, and also that our test data are limited so we may not have a very precise estimate of our accuracy. We can address these two problems with the same solution: cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "scores = sklearn.model_selection.cross_validate(clf, X, y, cv=5)\n",
    "print(scores)\n",
    "print(np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some different values of the `cv` paremeter here. What is it doing? Based on your results, what would you say a good estimate of our classifier's accuracy would be if it were trained on the *whole* dataset? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Support vector machines, and hyperparameter optimization\n",
    "Now let's try out a different type of classifier, the support vector machine, and see if we do any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma=0.1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but what are these `C` and `gamma` parameters? Actually, these are `hyperpameters`, kind of like normal parameters but more awesome. Actually this means that these are parameters governing the optimization of the actual model parameters (which are the parameters of the separating hyperplane and its surrounding margin---the support vectors).\n",
    "\n",
    "Note: `gamma` is a hyperparameter of the radial basis function (or Gaussian kernel), which is the most popular kernel function used with SVMs. Other kernel functions have other hyperparemters to optimize, while `C` is a hyperparameter of the SVM algorithm itself.\n",
    "\n",
    "When using SVMs, before optimizing our model parameters, it is recommended that we optimize our hyperparameters. How to do this? One possibility is to use a separate validation set, but if we are short on data, as is surely the case here, then we can once again rely on cross-validation. The key here is that we are only going to be looking at our training set to determine which hyperparameter combination performs best (hint: cross-validation). Once we have figured this out, then we can get an unbiased estimate of accuracy on our test set. I am going to leave this one as an exercise to you, but with a bit of help below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for C in [0.01, 0.1, 1, 10, 100]:\n",
    "    for gamma in [0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "        hyperparams=(C, gamma)\n",
    "        clf = svm.SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "        \n",
    "        #TODO: do some work here\n",
    "        \n",
    "        scores.append([meanCVScore, hyperparams])\n",
    "\n",
    "print(scores)\n",
    "\n",
    "#TODO: do some more work here\n",
    "\n",
    "print(bestHyperParams)\n",
    "bestC, bestGamma = bestHyperParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our best hyperparamter combination! Okay, that's not really true--all we did was a hyperparameter grid search, and there could be other combinations in between the points on this coarse hyperparameter grid that we tested, but grid searches tend to work well enough in practice for SVMs. So let's use this hyperparemter combination to train our SVM and see how well we do on our test set. Again, leaving this as an exercise to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Random forests, better accuracy metrics, and the best accuracy metrics (in my opinion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to try one last classification approach, random forests, and also take the opportunity to learn a few more accuracy metrics while we are at it. First, we have to import sklearn.ensemble. The module is so-named because random forests are an ensemble method--a better classifier is constructed from ensemble of potentially not-so-great classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y)\n",
    "\n",
    "clf = sklearn.ensemble.RandomForestClassifier() #let's start with default hyperperameters here\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quick and easy. Notice how I snuck in a re-load of the dataset but didn't bother with the standardization and yet we still did okay. Why do you think that is?\n",
    "\n",
    "I also haven't played around with the hyperparemeters at all---we are just using the defaults. If you end up having time, feel free to play around below with hyperparemeter optimization for your forest and see how much your scores vary--you may find the results illuminating about random forests. The documentation for how to do this is here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity, specificity, precision, and recall\n",
    "\n",
    "Let's refer to one of our classes (say, class 0) as negatives, and the rest (class 1) as positives. Definitions:\n",
    "\n",
    "tp: the number of true positives (class 1 examples correctly classified as such).\n",
    "fp: the number of false positives (class 0 examples incorrectly classified as class 1).\n",
    "tn: the number of true negatives (class 0 examples correctly classified as such).\n",
    "fp: the number of false negatives (class 1 examples incorrectly classified as class 0).\n",
    "\n",
    "sensitivity (aka recall): tp / (tp + fn)\n",
    "\n",
    "specificity: tn / (fp + tn)\n",
    "\n",
    "precision (aka positive predictive value): tp / (tp + fp)\n",
    "\n",
    "false discovery rate: 1 - precision = fp / (tp + fp)\n",
    "\n",
    "These metrics really only make sense for binary classifiers, so we will re-cast our problem of distinguishing one species of iris from the other two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newY = []\n",
    "for classLabel in y:\n",
    "    if classLabel == 0:\n",
    "        newY.append(0)\n",
    "    else:\n",
    "        newY.append(1)\n",
    "y = newY\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate each of the metrics above. Do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binClf = sklearn.ensemble.RandomForestClassifier()\n",
    "binClf.fit(X_train, y_train)\n",
    "preds = binClf.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# TODO: fill in the missing part here\n",
    "\n",
    "print(tp, fp, tn, fn)\n",
    "print(sensitivity, specificity, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and precision-recall curves\n",
    "Those metrics seem useful, but it is important to keep in mind that for any classifier we must impose a threshold on its *decision function*. For the SVM, this is essentially the distance from the separating hyperplane, for random forests, we get a posterior probability estimate of class membership. So really the numbers above only tell us about one potential classifier that can be made from the trained model---others can be made by changing our decision threshold.\n",
    "\n",
    "We can get a fuller picture of the potential discriminative power of a classification model by seeing how some of these metrics change according to our decision threshold, and that is what we will do below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "predScores = binClf.predict_proba(X_test)[:,1]\n",
    "\n",
    "def plotROCsAndPR(y_test, predScores):\n",
    "    fprs, tprs, threshes = sklearn.metrics.roc_curve(y_test, predScores)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, predScores)\n",
    "    precisions, recalls, threshes = sklearn.metrics.precision_recall_curve(y_test, predScores)\n",
    "    avgPrec = sklearn.metrics.average_precision_score(y_test, predScores)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fprs, tprs, label=\"RF (AUC = %.2f)\" %(auc), color=\"b\")\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve for random forest')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim((-0.05, 1.05))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recalls, precisions, label = \"RF (avg prec = %.2f)\" %(avgPrec), color=\"b\")\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim((-0.05, 1.05))\n",
    "    plt.show()\n",
    "\n",
    "plotROCsAndPR(y_test, predScores)\n",
    "#NOTE: scikit-learn now makes this even easier: see RocCurveDisplay and PrecisionRecallDisplay in the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that doesn't look very interesting, so let's try a more complex dataset.\n",
    "### Surprise detour into the Wisconsin Breast Cancer dataset\n",
    "\n",
    "The goal here is to discriminate between benign and malignant tumors on the basis of a number of measurements from the tumor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcX, bcy = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "bcX_train, bcX_test, bcy_train, bcy_test = train_test_split(bcX, bcy, test_size=0.4, stratify=bcy)\n",
    "\n",
    "binClf = sklearn.ensemble.RandomForestClassifier()\n",
    "binClf.fit(bcX_train, bcy_train)\n",
    "predScores = binClf.predict_proba(bcX_test)[:,1]\n",
    "plotROCsAndPR(bcy_test, predScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can see what our TP and FP rates are for all possible posterior probability thresholds. Think about what we want these curves to be shaped like.\n",
    "\n",
    "Fun note about ROC curves: the area under the curve (AUC) is the probability that any randomly selected positive example will have a higher decision function score (in this case posterior probability of membership in class one) than a randomly selected negative example.\n",
    "\n",
    "### Back to the iris data set, and confusion matrices\n",
    "\n",
    "Now let's go back to our three class iris data set and view the one metric to rule them all (at least for categorical data).\n",
    "\n",
    "First, we are going to define a function for plotting our output. You don't have to look closely at this, as the latest versions fo scikit-learn now have built-in functionality for this but I am including it to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's my confusion matrix function\n",
    "#sklearn now has it's own confusion matrix plotting utilities, but they differ by version\n",
    "#we are using this one intead just to ensure compatibility\n",
    "#best not to look at this code unless you really have to...\n",
    "def makeConfusionMatrixHeatmap(datain, title, trueClassOrderLs, predictedClassOrderLs, ax, normalize=True):\n",
    "    data = datain[:]\n",
    "    data.reverse()\n",
    "    fig,ax= plt.subplots(1,1)\n",
    "    data = np.array(data)\n",
    "    if normalize:\n",
    "        data = sklearn.preprocessing.normalize(data, axis=1, norm='l1')\n",
    "    heatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n",
    "\n",
    "    if normalize:\n",
    "        thresh = 50\n",
    "    else:\n",
    "        thresh = int(sum(data[0])/2)\n",
    "        print(thresh)\n",
    "    for i in range(len(predictedClassOrderLs)):\n",
    "        for j in reversed(range(len(trueClassOrderLs))):\n",
    "            val = data[j, i]\n",
    "            if normalize:\n",
    "                val *= 100\n",
    "            if val > thresh:\n",
    "                c = '0.9'\n",
    "            else:\n",
    "                c = 'black'\n",
    "            if normalize:\n",
    "                ax.text(i + 0.5, j + 0.5, '%.2f%%' % val, horizontalalignment='center',\n",
    "                        verticalalignment='center', color=c, fontsize=9)\n",
    "            else:\n",
    "                ax.text(i + 0.5, j + 0.5, '%d' % val, horizontalalignment='center',\n",
    "                        verticalalignment='center', color=c, fontsize=9)\n",
    "\n",
    "    cbar = plt.colorbar(heatmap, cmap=plt.cm.Blues, ax=ax)\n",
    "    cbar.set_label(\"Fraction of simulations assigned to class\", rotation=270, labelpad=20, fontsize=11)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "    ax.axis('tight')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    #labels\n",
    "    ax.set_xticklabels(predictedClassOrderLs, minor=False, fontsize=9, rotation=45)\n",
    "    ax.set_yticklabels(reversed(trueClassOrderLs), minor=False, fontsize=9)\n",
    "    ax.set_xlabel(\"Predicted class\")\n",
    "    ax.set_ylabel(\"True class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's take a look at this code below which computes and plots a more detailed summary of our classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y)\n",
    "\n",
    "clf = sklearn.ensemble.RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "#now the actual work\n",
    "#first get the predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(y_test)):\n",
    "    counts[y_test[i]][y_pred[i]] += 1\n",
    "\n",
    "#now do the plotting\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", class_names, class_names, ax, normalize=False)\n",
    "plt.show()\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", class_names, class_names, ax, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a careful look and make sure you understand what is being shown here. Although this is not the reason for the name, some find this plot confusing. But spend a bit of time with it (discussing with friends if needed) and you should see that not only is this plot fairly straightforward, it actually contains all of the information we would like to know about our classifier (unless we want to tweak classification thresholds in which case ROC and precision-recall curves are handy).\n",
    "\n",
    "Note the difference between the two versions of the plot above. Which one do you think would be more useful in which situations? (Think about class imbalances, test data set sizes, etc).\n",
    "\n",
    "If you find yourself with some extra time, you may wish to compare to another classifier (e.g. SVM). So plot a confusion matrix for that one too if you like. Come to think of it, how is it possible that an SVM, which draws a hyperplane between two classes of data, can be used for a 3-way classification problem like this anyway? (If curious see: https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert optional code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "That's it! Hopefully this was a helpful demonstration of how to train a few classifiers in scikit-learn, the importance of hyperparameter optimization, and most importantly, how to evaluate model performance. Hit me up on slack or email (drs@unc.edu) if you have any questions that I didn't get a chance to answer during the exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
